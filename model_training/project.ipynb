{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from matplotlib.image import imread\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "seed = 42\n",
    "sns.set()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read and preprocessed the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "data_path = Path('../input/retail-product-checkout-dataset')\n",
    "csv_path = Path('../input/rpc-df3/rpc_train_dataframe_super.csv')\n",
    "\n",
    "data = pd.read_csv(csv_path)\n",
    "data = data.rename(columns={'supercategory_id':'labels'})\n",
    "data = data.rename(columns={'bbox':'bboxes'})\n",
    "print(f'Column names: {list(data)}')\n",
    "data['labels'].astype('int64')\n",
    "data = data.set_index('image_id')\n",
    "mapping = data.groupby(['labels','supercategory']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "\n",
    "train_data, val_data = train_test_split(data,\n",
    "                                        test_size=0.1, shuffle=True, random_state=seed)\n",
    "num_classes = train_data['labels'].nunique()\n",
    "print(f'There are {num_classes} classes in training data')\n",
    "mapping\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def text_label(index):\n",
    "    return mapping.loc[mapping['labels'] == index, 'supercategory'].iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Create Pytorch dataset**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from albumentations import BboxParams, Compose, OneOf, Resize, RGBShift, \\\n",
    "    RandomBrightnessContrast, Rotate, \\\n",
    "    Normalize, ShiftScaleRotate, HorizontalFlip, VerticalFlip, CenterCrop, \\\n",
    "    RandomSizedBBoxSafeCrop, Blur, ColorJitter, RandomGamma, \\\n",
    "    Cutout\n",
    "from albumentations.augmentations.bbox_utils import convert_bbox_from_albumentations\n",
    "from albumentations.augmentations.transforms import Equalize\n",
    "from albumentations.imgaug.transforms import IAAAffine\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "image_height, image_width = 128, 128\n",
    "\n",
    "dataset_format = 'pascal_voc'\n",
    "bbox_params=BboxParams(format=dataset_format,\n",
    "                       min_visibility=0.3,\n",
    "                       label_fields=['labels'])\n",
    "\n",
    "train_augment = Compose([\n",
    "                          OneOf([\n",
    "                              RGBShift(r_shift_limit=15, g_shift_limit=15,\n",
    "                                       b_shift_limit=15, p=0.5),\n",
    "                              RandomBrightnessContrast(brightness_limit=0.1,\n",
    "                                                       contrast_limit=0.05),\n",
    "                              Equalize(p=0.1),\n",
    "                              RandomGamma(p=0.5),\n",
    "                              ColorJitter(p=0.5),\n",
    "                          ], p=0.9),\n",
    "                          OneOf([\n",
    "                              ShiftScaleRotate(shift_limit=0.1625,\n",
    "                                               scale_limit=0.1,\n",
    "                                               rotate_limit=20,\n",
    "                                               interpolation=1,\n",
    "                                               border_mode=4, p=0.9),\n",
    "                          ], p=0.6),\n",
    "\n",
    "                          OneOf([\n",
    "                              Blur(blur_limit=3, p=0.1),\n",
    "                              Cutout(num_holes=8, max_h_size=5,\n",
    "                                     max_w_size=5, fill_value=0, p=0.3),\n",
    "                              \n",
    "                          ], p=0.2),\n",
    "                        ],\n",
    "                         bbox_params=bbox_params)\n",
    "\n",
    "transform = Compose([ \n",
    "    Resize(image_height, image_width),\n",
    "    Normalize(mean=(0, 0, 0), std=(1, 1, 1)),\n",
    "    ToTensorV2(),\n",
    "], bbox_params=bbox_params)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, data_dir, images_info_df, augment=None, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data = images_info_df \n",
    "        self.augment = augment\n",
    "        self.transform = transform\n",
    "\n",
    "    def check_min(self, value):\n",
    "        if value > 0:\n",
    "            return value\n",
    "        return 0\n",
    "    \n",
    "    def check_max(self, value, max_value):\n",
    "        if value < max_value:\n",
    "            return value\n",
    "        return max_value\n",
    "    \n",
    "    def resize(self, image, bboxes, env_size=0.2):\n",
    "        [x, y, w, h] = bboxes\n",
    "        im_h, im_w, im_c = image.shape\n",
    "\n",
    "        if w > h:\n",
    "            x_max = int(self.check_max(x + (w * env_size) + w, im_w))\n",
    "            x_min = int(self.check_min(x - (w * env_size)))\n",
    "            new_w = (x_max - x_min) / 2\n",
    "            y_mid = (y + (y + h)) / 2\n",
    "            y_max = int(self.check_max(y_mid + new_w, im_h))\n",
    "            y_min = int(self.check_min(y_mid - new_w))\n",
    "        else:\n",
    "            y_max = int(self.check_max(y + (h * env_size) + h, im_h))\n",
    "            y_min = int(self.check_min(y - (h * env_size)))\n",
    "            new_h = (y_max - y_min) / 2 \n",
    "            x_mid = (x + (x + w)) / 2\n",
    "            x_max = int(self.check_max(x_mid + new_h, im_w))\n",
    "            x_min = int(self.check_min(x_mid - new_h))\n",
    "            \n",
    "        if y_min < 0 or x_min < 0:\n",
    "            print(x, y, w, h, x_min, y_min, x_max, y_max)\n",
    "            print(bboxes)\n",
    "            \n",
    "        image = image[y_min:y_max, x_min:x_max, :]\n",
    "        bboxes = np.array([x - x_min, y - y_min, w, h])\n",
    "        return image, bboxes\n",
    "    \n",
    "    def convert_format(self, res):\n",
    "        xmin, ymin, width, height = res[\"bboxes\"][0], res[\"bboxes\"][1], \\\n",
    "                res[\"bboxes\"][2], res[\"bboxes\"][3] \n",
    "        xmax, ymax = xmin + width, ymin + height\n",
    "        img_h, img_w = res['image'].shape[0], res['image'].shape[1]\n",
    "        res[\"bboxes\"] = convert_bbox_from_albumentations(bbox=(xmin / img_w,\n",
    "                                                               ymin / img_h,\n",
    "                                                               xmax / img_w,\n",
    "                                                               ymax / img_h),\n",
    "                                target_format=dataset_format,\n",
    "                                rows=img_h, cols=img_w, check_validity=True)\n",
    "        return res\n",
    "\n",
    "    def __getitem__(self, i, apply_augmentations=True,\n",
    "                    apply_transformations=True):\n",
    "        record = self.data.iloc[i]\n",
    "\n",
    "        image = cv2.imread(str(self.data_dir / record['file_name']), 1)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        bboxes = np.array(record['bboxes'][1:-1].split(',')).astype('float')\n",
    "\n",
    "        image, bboxes = self.resize(image, bboxes)\n",
    "\n",
    "        labels = np.array(record['labels']).astype('int64')\n",
    "\n",
    "        res = {\"image\": image,\n",
    "               \"bboxes\": torch.tensor(bboxes, dtype=torch.float),\n",
    "               \"labels\": torch.tensor(np.array([labels])),\n",
    "              }\n",
    "        # convert bounding boxes to 'pascal_voc' format\n",
    "        res = self.convert_format(res)\n",
    "        res[\"bboxes\"] = torch.tensor(res[\"bboxes\"]).reshape((1, -1))\n",
    "\n",
    "        if self.augment and apply_augmentations:\n",
    "            res = self.augment(**res)\n",
    "\n",
    "        if self.transform and apply_transformations:\n",
    "            res = self.transform(**res)\n",
    "\n",
    "        return res[\"image\"], {\n",
    "            \"boxes\": torch.tensor(res[\"bboxes\"], dtype=torch.float),\n",
    "            \"labels\": torch.tensor(res[\"labels\"], dtype=torch.int64),\n",
    "        }   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = ProductDataset(data_path / 'train2019', train_data,\n",
    "                              augment=train_augment, transform=transform)\n",
    "val_dataset = ProductDataset(data_path / 'train2019', val_data,\n",
    "                              augment=None, transform=transform)\n",
    "print(len(train_dataset), len(val_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Visualize dataset**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def get_grid_size(imgs_num, nrows, ncols):\n",
    "    from math import ceil\n",
    "    if nrows is None and ncols is None:\n",
    "        nrows = 1\n",
    "        ncols = imgs_num\n",
    "    elif nrows is None:\n",
    "        nrows = ceil(imgs_num / ncols)\n",
    "    elif ncols is None:\n",
    "        ncols = ceil(imgs_num / nrows)\n",
    "    return nrows, ncols\n",
    "\n",
    "def plot_detection_boxes(img, detection_boxes, detection_labels,\n",
    "                         dataset_format=None, verbose=False):\n",
    "    if verbose:\n",
    "        print(f'Dislay in {dataset_format} format')\n",
    "    # `coco` format: `(x_min, y_min, width, height)`\n",
    "    # `pascal_voc`format:`(x_min, y_min, x_max, y_max)`\n",
    "    # `yolo` format :`(x, y, width, height)`\n",
    "    for box, label in zip(detection_boxes, detection_labels):\n",
    "        if dataset_format == 'coco':\n",
    "            # round values only for visualization (opencv requirement)\n",
    "            xmin, ymin, width, height = int(box[0]), int(box[1]), \\\n",
    "                                        int(box[2]), int(box[3])\n",
    "            img = cv2.rectangle(np.array(img), (xmin, ymin + height),\n",
    "                                (xmin + width, ymin), (0,255,0), 3)\n",
    "        elif dataset_format == 'pascal_voc':\n",
    "            xmin, ymin, xmax, ymax = int(box[0]), int(box[1]), \\\n",
    "                                     int(box[2]), int(box[3])\n",
    "            img = cv2.rectangle(np.array(img), (xmin, ymax), (xmax, ymin),\n",
    "                                (0,255,0), 3)\n",
    "        else:\n",
    "            raise ValueError(f'Please specify a format of bounding boxes as '\n",
    "                             f'dataset_format parameter. Currently supported '\n",
    "                             f'formats are \"coco\" and \"pascal_voc\"')\n",
    "        scale = 0.3 # (0,1] to change text size relative to the image\n",
    "        fontScale = min(img.shape[1],img.shape[0])/(150/scale)\n",
    "        img = cv2.putText(img, text=str(label), org=(xmin, ymin),\n",
    "                          fontFace=3, fontScale=fontScale, color=(0, 0, 0))\n",
    "    return img\n",
    "\n",
    "\n",
    "def plot_images(imgs, names=None, axs=None, show=True, nrows=None, ncols=None,\n",
    "                figsize=(16, 8), mode=None, detection_boxes=None,\n",
    "                detection_labels=None, dataset_format=None, pic_name=None):\n",
    "    nrows, ncols = get_grid_size(len(imgs), nrows, ncols)\n",
    "\n",
    "    if axs is None:\n",
    "        fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=figsize)\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        if mode == 'detection':\n",
    "            imgs[0] = plot_detection_boxes(imgs[0], detection_boxes,\n",
    "                                           detection_labels,\n",
    "                                           dataset_format=dataset_format)\n",
    "        axs.imshow(imgs[0])\n",
    "        axs.set_axis_off()\n",
    "        if names and len(names) > 0:\n",
    "            axs.set_title(names[0], fontsize=15)\n",
    "        if pic_name:\n",
    "            pass\n",
    "            # axs.imsave(f'{pic_name}.jpg', imgs[0])\n",
    "    elif nrows == 1 or ncols == 1:\n",
    "        for j, ax in enumerate(axs):\n",
    "            if mode == 'detection':\n",
    "                if ncols == 1:\n",
    "                    imgs[j] = plot_detection_boxes(imgs[j],\n",
    "                                                   [detection_boxes[j]],\n",
    "                                                   [detection_labels[j]],\n",
    "                                                dataset_format=dataset_format)\n",
    "                elif nrows == 1:\n",
    "                    imgs[j] = plot_detection_boxes(imgs[j],\n",
    "                                                   [detection_boxes[j]],\n",
    "                                                   [detection_labels[j]],\n",
    "                                               dataset_format=dataset_format)\n",
    "            ax.imshow(imgs[j])\n",
    "            ax.set_axis_off()\n",
    "            if names and j < len(names):\n",
    "                ax.set_title(names[j], fontsize=15)\n",
    "    else:\n",
    "        for j, ax in enumerate(axs):\n",
    "            for k, sub_ax in enumerate(ax):\n",
    "                image_id = j * ncols + k\n",
    "                sub_ax.set_axis_off()\n",
    "                if image_id < len(imgs):\n",
    "                    if mode == 'detection':\n",
    "                        imgs[image_id] = plot_detection_boxes(imgs[image_id],\n",
    "                                                [detection_boxes[image_id]],\n",
    "                                                [detection_labels[image_id]],\n",
    "                                                dataset_format=dataset_format)\n",
    "                    sub_ax.imshow(imgs[image_id])        \n",
    "                    if names and image_id < len(names):\n",
    "                        sub_ax.set_title(names[image_id], fontsize=15)\n",
    "    if show:\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def show_dataset(dataset, sample_size=3, n=6, show_augmentations=True):\n",
    "    idx = [np.random.choice(np.arange(dataset.__len__())) for i in range(sample_size)]\n",
    "    imgs = []\n",
    "    names = ['original']\n",
    "    detection_boxes = []\n",
    "    detection_labels = []\n",
    "\n",
    "    for index in idx:\n",
    "        item = dataset.__getitem__(index, apply_augmentations=False,\n",
    "                                   apply_transformations=False)\n",
    "        img = item[0]\n",
    "        annotations_true = item[1]\n",
    "        true_boxes, true_labels = list(annotations_true['boxes'].numpy()[0]), \\\n",
    "                                  list(annotations_true['labels'].numpy())[0]\n",
    "        label_names = text_label(true_labels)\n",
    "        imgs.append(img)\n",
    "        detection_boxes.append(true_boxes)\n",
    "        detection_labels.append(label_names)\n",
    "\n",
    "        if show_augmentations:\n",
    "            for _ in range(n-1):\n",
    "                augmented = dataset.__getitem__(index,\n",
    "                                                apply_augmentations=True,\n",
    "                                                apply_transformations=False)\n",
    "                img_augmented = augmented[0]\n",
    "                if len(augmented) > 1 and len(augmented[1][\"boxes\"]) > 0:\n",
    "                    annotations = augmented[1]\n",
    "                    boxes_augmented = list(augmented[1][\"boxes\"].numpy()[0])\n",
    "                    labels_augmented = list(augmented[1][\"labels\"].numpy())[0]\n",
    "                    label_names = text_label(labels_augmented)\n",
    "\n",
    "                    imgs.append(img_augmented)\n",
    "                    detection_boxes.append(boxes_augmented)\n",
    "                    detection_labels.append(label_names)\n",
    "    if show_augmentations:\n",
    "        aug = ['augmented' for _ in range(n-1)]\n",
    "        names = names + aug\n",
    "    plot_images(imgs, nrows=sample_size, ncols=n, mode='detection',\n",
    "                names=names, detection_boxes=detection_boxes,\n",
    "                detection_labels=detection_labels,\n",
    "                dataset_format=dataset_format)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_dataset(train_dataset, sample_size=4, n=3) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "show_dataset(val_dataset, sample_size=2, n=1) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Code for model performance evaluation and training\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize():\n",
    "    \"\"\"\n",
    "    This function will only execute if `DEBUG` is `True` in \n",
    "    `config.py`.\n",
    "    \"\"\"\n",
    "    images, targets, image_ids = next(iter(train_data_loader))\n",
    "    images = list(image for image in images)\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "    for i in range(1):\n",
    "        boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n",
    "        sample = images[i].permute(1,2,0).cpu().numpy()\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 12))\n",
    "        for box in boxes:\n",
    "            cv2.rectangle(sample,\n",
    "                        (box[0], box[1]),\n",
    "                        (box[2], box[3]),\n",
    "                        (220, 0, 0), 3)\n",
    "        ax.set_axis_off()\n",
    "        plt.imshow(sample)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def intersection_over_union(dt_bbox, gt_bbox):\n",
    "    \"\"\"\n",
    "    Intersection over Union between two bboxes\n",
    "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1], i.e. xmin, ymin, xmax, ymax\n",
    "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :return : intersection over union\n",
    "    \"\"\"\n",
    "\n",
    "    intersection_bbox = np.array(\n",
    "        [\n",
    "            max(dt_bbox[0], gt_bbox[0]),\n",
    "            max(dt_bbox[1], gt_bbox[1]),\n",
    "            min(dt_bbox[2], gt_bbox[2]),\n",
    "            min(dt_bbox[3], gt_bbox[3]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
    "        intersection_bbox[3] - intersection_bbox[1], 0\n",
    "    )\n",
    "    area_dt = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n",
    "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "\n",
    "    union_area = area_dt + area_gt - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
    "    \n",
    "    gt_bboxes = target_true[\"boxes\"].numpy()  # ground truth \n",
    "    gt_labels = target_true[\"labels\"].numpy()\n",
    "\n",
    "    dt_bboxes = target_pred[\"boxes\"].numpy()  # detected \n",
    "    dt_labels = target_pred[\"labels\"].numpy()\n",
    "    \n",
    "    dt_scores = target_pred[\"scores\"].numpy()\n",
    "\n",
    "    results = []\n",
    "    for detection_id in range(len(dt_labels)):\n",
    "        dt_bbox = dt_bboxes[detection_id, :]\n",
    "        dt_label = dt_labels[detection_id]\n",
    "        dt_score = dt_scores[detection_id]\n",
    "        \n",
    "        detection_result_dict = {\"score\": dt_score}\n",
    "\n",
    "        max_IoU = 0\n",
    "        max_gt_id = -1\n",
    "        for gt_id in range(len(gt_labels)): \n",
    "            gt_bbox = gt_bboxes[gt_id, :]\n",
    "            gt_label = gt_labels[gt_id]\n",
    "            \n",
    "            if gt_label != dt_label:\n",
    "                continue\n",
    "            \n",
    "            iou_current = intersection_over_union(dt_bbox, gt_bbox)\n",
    "            if iou_current > max_IoU:\n",
    "                max_IoU = iou_current\n",
    "                max_gt_id = gt_id\n",
    "\n",
    "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
    "            detection_result_dict[\"TP\"] = 1\n",
    "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
    "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
    "\n",
    "        else:\n",
    "            detection_result_dict[\"TP\"] = 0\n",
    "\n",
    "        results.append(detection_result_dict)\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, test_loader, device='cuda'):\n",
    "    results = [] \n",
    "    model.eval()\n",
    "    nbr_boxes = 0\n",
    "    num_batches = len(test_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets_true in tqdm(test_loader, total=num_batches):\n",
    "            images = torch.stack(images).to(device)\n",
    "            targets_true = [{k: v.to(device) for k, v in t.items()} for t in targets_true]\n",
    "                    \n",
    "            targets_pred = model(images)\n",
    "        \n",
    "            targets_true = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_true\n",
    "            ]\n",
    "            targets_pred = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_pred\n",
    "            ]\n",
    "\n",
    "            for i in range(len(targets_true)): \n",
    "                target_true = targets_true[i] \n",
    "                target_pred = targets_pred[i]\n",
    "                nbr_boxes += target_true[\"labels\"].shape[0]\n",
    "\n",
    "                results.extend(evaluate_sample(target_pred, target_true))\n",
    "    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "    acc_TP = np.zeros(len(results))\n",
    "    acc_FP = np.zeros(len(results))\n",
    "    recall = np.zeros(len(results))\n",
    "    precision = np.zeros(len(results))\n",
    "    \n",
    "    if results is not None: \n",
    "        if results[0][\"TP\"] == 1:\n",
    "            acc_TP[0] = 1\n",
    "        else:\n",
    "            acc_FP[0] = 1\n",
    "\n",
    "    for i in range(1, len(results)):\n",
    "        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n",
    "        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n",
    "\n",
    "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
    "        recall[i] = acc_TP[i] / nbr_boxes\n",
    "\n",
    "    return auc(recall, precision)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, train_dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    pbar = tqdm(enumerate(train_dataloader), total=num_batches)\n",
    "    for i, (images, targets) in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step() # Cyclic LRs after each batch\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_description_str(f'loss: {running_loss / (i+1):.3f}',\n",
    "                                 refresh=False)\n",
    "    train_loss = running_loss / len(train_dataloader.dataset)\n",
    "    return train_loss\n",
    "\n",
    "def save_model(model_save_path):\n",
    "    torch.save(model.state_dict(), Path(model_save_path))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def get_detection_model(num_classes, pretrained=True):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=pretrained, min_size=256, max_size=448)\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,\n",
    "    collate_fn=collate_fn)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2,\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "print(len(train_dataloader), len(train_dataset))\n",
    "print(len(val_dataloader), len(val_dataset))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train a model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = get_detection_model(num_classes+1, pretrained=True).to(device)\n",
    "params = [v for k, v in model.named_parameters() if v.requires_grad]\n",
    "\n",
    "max_lr = 1e-1\n",
    "optimizer = torch.optim.SGD(params, lr=max_lr, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# # Train for 1 epoch to make model's predictions more confident\n",
    "# n_epochs = 1\n",
    "# for epoch in range(n_epochs):\n",
    "#     start = time.time()\n",
    "#     train_loss = train(model, optimizer, train_dataloader)\n",
    "#     print(f\"Epoch #{epoch} loss: {train_loss}\")\n",
    "#     end = time.time()\n",
    "#     print(f\"Took {(end - start) / 60} minutes for epoch {epoch}\")\n",
    "# save_model('fasterrcnn_resnet50_fpn_1_epoch.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train more\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(params, lr=max_lr, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "num_batches_train = len(train_dataloader)\n",
    "n_epochs = 5\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, epochs=n_epochs, steps_per_epoch=num_batches_train) \n",
    "\n",
    "# todo remove commented\n",
    "# model_path = Path('../input/last-of-mohecan/fasterrcnn_resnet50_fpn_mon.pth')\n",
    "model_path = Path('fasterrcnn_resnet50_fpn_1_epoch.pth')\n",
    "model = get_detection_model(num_classes+1, pretrained=False).to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "path_to_model_new = Path('fasterrcnn_resnet50_fpn_submit.pth')\n",
    "\n",
    "model = model.to(device)\n",
    "best_val_auc = 0.0\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.eval()\n",
    "    val_auc = evaluate(model, val_dataloader, device=device)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        save_model(path_to_model_new)\n",
    "    print(f'Epoch {epoch}. AUC ON TEST: {round(float(val_auc), 4)}')\n",
    "    \n",
    "    model.train() \n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "num_batches = len(val_dataloader)\n",
    "images, targets_true, outputs = [], [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_i, (valid_images, targ_true) in tqdm(enumerate(val_dataloader), total=num_batches):\n",
    "        imgs = torch.stack(valid_images).to(device)\n",
    "        \n",
    "        images += [image for image in valid_images]\n",
    "        targets_true += [{k: v.to(device) for k, v in t.items()} for t in targ_true]\n",
    "\n",
    "        cpu_device = torch.device(\"cpu\")\n",
    "        outs = model(imgs)\n",
    "        \n",
    "        outputs += [{k: v.to(cpu_device) for k, v in t.items()} for t in outs]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "detection_threshold = 0.4\n",
    "\n",
    "counter = 0\n",
    "hit = 0\n",
    "miss, miss_score, miss_detect = 0, 0, 0\n",
    "\n",
    "true_labs = []\n",
    "predicted_labs = []\n",
    "\n",
    "for i in range(len(targets_true)):\n",
    "    orig_image = images[i]\n",
    "    target_true = targets_true[i]\n",
    "    predicted = outputs[i]\n",
    "    scores = predicted['scores']\n",
    "    counter += 1\n",
    "    if len(scores) > 0:\n",
    "        max_index = scores.argmax()\n",
    "        max_score = scores[max_index]\n",
    "\n",
    "        if max_score >= detection_threshold:\n",
    "            predicted_label = predicted['labels'][max_index].to(device=\"cpu\")\n",
    "            true_label = target_true['labels'][0].to(device=\"cpu\")\n",
    "\n",
    "            predicted_labs.append(int(predicted_label.numpy()))\n",
    "            true_labs.append(int(true_label.numpy()))\n",
    "\n",
    "            pred_label_text = text_label(int(predicted_label.numpy()))\n",
    "            true_label_text = text_label(int(true_label.numpy()))\n",
    "\n",
    "            if int(predicted_label) == int(true_label):\n",
    "                hit += 1\n",
    "            else:\n",
    "                miss +=1\n",
    "\n",
    "            box = predicted['boxes'][max_index]\n",
    "\n",
    "            if counter % 100 == 0:\n",
    "                plot_images([orig_image.cpu().numpy().transpose(1, 2, 0)], \n",
    "                            nrows=1, ncols=1, mode='detection', \n",
    "                            detection_boxes=[box], \n",
    "                            detection_labels=[f\"{pred_label_text}\"],\n",
    "                            pic_name=f\"{counter}\",\n",
    "                            dataset_format='pascal_voc')\n",
    "\n",
    "        else:\n",
    "            miss_score += 1\n",
    "    else:\n",
    "        miss_detect += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_labels = mapping['labels'].tolist()\n",
    "text_labels = mapping['supercategory'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Overall accuracy: {hit / counter}\\nMiss on not detected: {miss_detect / counter}\")\n",
    "print(f\"Miss on low enssurance score: {miss_score / counter}\\nMiss on wrong prediction: {miss / counter}\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def make_confusion_matrix(cf,\n",
    "                          file_name,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                   \n",
    "    title:         Title for the heatmap. Default is None.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "#         group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "        group_percentages = []\n",
    "        group_percentages_num = []\n",
    "        for row in cf:\n",
    "            for value in row / np.sum(row):\n",
    "                group_percentages.append(\"{0:.1%}\".format(value))\n",
    "                group_percentages_num.append(value)\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "        stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    group_percentages_num = np.asarray(group_percentages_num).reshape(cf.shape[0],cf.shape[1])\n",
    "    sns.heatmap(group_percentages_num,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.savefig(file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cf_matrix = confusion_matrix(true_labs, predicted_labs, normalize=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "make_confusion_matrix(cf_matrix,\n",
    "                      f'cf_th{detection_threshold}.png',\n",
    "                      categories=text_labels,\n",
    "                      figsize=(15, 13),\n",
    "                      cmap='rocket',\n",
    "                      title=f\"Confusion matrix for {detection_threshold} detection threshold\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}